{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neighborhood': 'forest shadows', 'address': '4706 Broadmark Lane', 'city': 'Humble', 'state': 'TX', 'zip': '77338', 'days': '15', 'agent': 'Cellina Stokes', 'office': 'Crowne Realty'}\n",
      "{'neighborhood': 'forest shadows', 'address': '4602 Fieldwick Lane', 'city': 'Humble', 'state': 'TX', 'zip': '77338', 'days': '29', 'agent': 'Michael Thiltgen', 'office': 'Greenbriar Real Estate Service'}\n",
      "{'neighborhood': 'forest shadows', 'address': '21802 Pine Cone Drive', 'city': 'Humble', 'state': 'TX', 'zip': '77338', 'days': '34', 'agent': 'Katherine Ware', 'office': 'Irie Properties, LLC'}\n",
      "{'neighborhood': 'forest shadows', 'address': '4503 Clairfield Lane', 'city': 'Humble', 'state': 'TX', 'zip': '77338', 'days': '71', 'agent': 'Rick Raymor', 'office': 'RE/MAX Professional Group     '}\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "from urllib import parse\n",
    "import re \n",
    "from datetime import datetime\n",
    "import schedule\n",
    "import threading \n",
    "from task import homes_comparison\n",
    "import csv\n",
    "\n",
    "\n",
    "urls = [\"https://www.har.com/forest-shadows/homes-for-sale/4585?sort=listdate+desc\"]\n",
    "\n",
    "all_homes = []\n",
    "\n",
    "#iterate through the urls\n",
    "for url in urls:\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    homes = soup.find('div', class_= \"prop_list\").text\n",
    "    neighborhood = url.split('/')[3].replace('-',' ')\n",
    "    home_info = dict()\n",
    "    home_info ['neighborhood'] = neighborhood\n",
    "\n",
    "\n",
    "    try:\n",
    "        for info in soup.findAll(class_= 'mpi_info'):\n",
    "            full_address = info.findAll('a', class_= 'address')[0].text\n",
    "            split_address = full_address.split(',')\n",
    "            home_info['address']= split_address[0].strip()\n",
    "            home_info['city']= split_address[1].strip()\n",
    "            state_zip = split_address[2].split(' ')\n",
    "            home_info['state'] = state_zip[1].strip()\n",
    "            home_info['zip'] = state_zip[2].strip()\n",
    "            home_info['days']= info.findAll('span', class_= 'bold')[1].text\n",
    "            home_info['agent'] = info.findAll('a', class_='bold')[0].text\n",
    "            home_info['office'] =info.findAll('a', class_= 'bold')[1].text\n",
    "            print(home_info)\n",
    "            \n",
    "            \n",
    "#         price =[img.text.replace(',','').replace('$','').strip() for img in soup.findAll(class_= 'mpi_img')]\n",
    "#         home_info['price']= price \n",
    "    except:\n",
    "        print(\"No results given for \" + url)\n",
    "        continue\n",
    "    \n",
    " \n",
    "    print(all_homes)\n",
    "\n",
    "#     with open('scraped_homes.csv', 'w', encoding='utf8', newline='') as output_file:\n",
    "#         fc = csv.DictWriter(output_file, \n",
    "#                             fieldnames=all_homes[1:5].keys(),\n",
    "\n",
    "#                            )\n",
    "#         fc.writeheader()\n",
    "#         fc.writerows(all_homes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
