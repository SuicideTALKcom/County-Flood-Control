{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n  175000   \\nFor Sale\\n\\n\\n\\n\\n', '\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n  189900   \\nFor Sale\\n\\n\\n\\n\\n', '\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n  249450   \\nFor Sale\\n\\n\\n\\n\\n', '\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n  264500   \\nFor Sale\\n\\n\\n\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "from urllib import parse\n",
    "import re \n",
    "from datetime import datetime\n",
    "import schedule\n",
    "import threading \n",
    "# from task import homes_comparison\n",
    "import csv\n",
    "\n",
    "\n",
    "urls = [\"https://www.har.com/forest-shadows/homes-for-sale/4585?sort=listdate+desc\"]\n",
    "\n",
    "all_homes = []\n",
    "\n",
    "#iterate through the urls\n",
    "for url in urls:\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    homes = soup.find('div', class_= \"prop_list\").text\n",
    "    neighborhood = url.split('/')[3].replace('-',' ')\n",
    "    home_info = dict()\n",
    "    home_info ['neighborhood'] = neighborhood\n",
    "\n",
    "\n",
    "    try:\n",
    "        for info in soup.findAll(class_= 'mpi_info'):\n",
    "            full_address = info.findAll('a', class_= 'address')[0].text\n",
    "            split_address = full_address.split(',')\n",
    "            home_info['address']= split_address[0].strip()\n",
    "            home_info['city']= split_address[1].strip()\n",
    "            state_zip = split_address[2].split(' ')\n",
    "            home_info['state'] = state_zip[1].strip()\n",
    "            home_info['zip'] = state_zip[2].strip()\n",
    "            home_info['days']= info.findAll('span', class_= 'bold')[1].text\n",
    "            home_info['agent'] = info.findAll('a', class_='bold')[0].text\n",
    "            home_info['office'] =info.findAll('a', class_= 'bold')[1].text\n",
    "#             print(home_info)\n",
    "            price =[img.text.replace(',','').replace('$','').strip('') for img in soup.findAll(class_= 'mpi_img')]\n",
    "            home_info['price']= price\n",
    "    except:\n",
    "        print(\"No results given for \" + url)\n",
    "        continue\n",
    "    \n",
    " \n",
    "    print(home_info['price'])\n",
    "\n",
    "#     with open('scraped_homes.csv', 'w', encoding='utf8', newline='') as output_file:\n",
    "#         fc = csv.DictWriter(output_file, \n",
    "#                             fieldnames=all_homes[1:5].keys(),\n",
    "\n",
    "#                            )\n",
    "#         fc.writeheader()\n",
    "#         fc.writerows(all_homes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
